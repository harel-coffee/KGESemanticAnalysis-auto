{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiments for Classification\n",
    "\n",
    "1. Read embedding and classes\n",
    "2. Build data format for multi-label classification problem\n",
    "3. train-test split\n",
    "4. Experiments:\n",
    "- MLP Classifier\n",
    "- Random Forest\n",
    "- KNN \n",
    "\n",
    "5. Add SDType information manually\n",
    "6. Draw diagrams with seaborn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import torch\n",
    "from kge.model import KgeModel\n",
    "from kge.util.io import load_checkpoint\n",
    "import numpy as np\n",
    "from sklearn.metrics import balanced_accuracy_score, precision_recall_fscore_support, precision_score, recall_score, f1_score\n",
    "\n",
    "from imblearn.pipeline import make_pipeline\n",
    "from imblearn.over_sampling import ADASYN\n",
    "from imblearn.over_sampling import (SMOTE, BorderlineSMOTE, SVMSMOTE, SMOTENC,\n",
    "                                    KMeansSMOTE)\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.base import BaseSampler\n",
    "from imblearn.metrics import classification_report_imbalanced\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import genfromtxt\n",
    "\n",
    "\n",
    "def load_embedding(path):\n",
    "    if \"rdf2vec\" in path:\n",
    "        entities = []\n",
    "        embedding = genfromtxt(path+\".tsv\", delimiter=',')\n",
    "        if len(embedding) == 0:\n",
    "            print(\"Reading RDF2Vec embedding is incorrect\")\n",
    "        with open(path+\"-entities.tsv\", \"r\") as rdf2vec_entity_file:\n",
    "            for line in rdf2vec_entity_file:\n",
    "                entities.append(line.replace(\"http://www.freebase.com\",\"\").replace(\"http://www.yago-knowledge.org/\",\"\").replace(\"\\n\",\"\"))\n",
    "        return embedding, entities\n",
    "    \n",
    "    else:\n",
    "        #load YAGO Complex Embedding\n",
    "        checkpoint = load_checkpoint(path)\n",
    "        model = KgeModel.create_from(checkpoint)\n",
    "        #get entity embedding\n",
    "        train = torch.Tensor(range(0, model.dataset.num_entities())).long() \n",
    "        #get list of entity IDs\n",
    "        entity_list = model.dataset.entity_ids(train)\n",
    "        \n",
    "    return model, entity_list\n",
    "\n",
    "\n",
    "\n",
    "type_file_path = \"/yago3-10/yagoTransitiveType.tsv\"\n",
    "def get_types(type_file_path, ds):\n",
    "    if ds == \"Yago\":\n",
    "        #read yago transitive types\n",
    "        from collections import defaultdict\n",
    "        class_entity_dict = defaultdict(set)\n",
    "        entity_class_dict = defaultdict(set)\n",
    "        with open(type_file_path, \"r\") as yago_types:\n",
    "            for line in yago_types:\n",
    "                try:\n",
    "                    x, entity, predicate, cl = line.split()\n",
    "                    entity = entity.replace(\">\",\"\").replace(\"<\",\"\")\n",
    "                    cl = cl.replace(\">\",\"\").replace(\"<\",\"\")\n",
    "                    class_entity_dict[cl].add(entity)\n",
    "                    entity_class_dict[entity].add(cl)\n",
    "                except ValueError:\n",
    "                    continue\n",
    "    if ds == \"Freebase\":\n",
    "        from collections import defaultdict\n",
    "        class_entity_dict = defaultdict(set)\n",
    "        entity_class_dict = defaultdict(set)\n",
    "        with open(type_file_path, \"r\") as fb_types:\n",
    "            for line in fb_types:\n",
    "                try:\n",
    "                    entity, cl = line.split()\n",
    "                    class_entity_dict[cl].add(entity)\n",
    "                    entity_class_dict[entity].add(cl)\n",
    "                except ValueError:\n",
    "                    continue\n",
    "        \n",
    "    return class_entity_dict, entity_class_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiments\n",
    "\n",
    "We evaluate the classification score of Sci-Kit clf.score(). \n",
    "This reflects the mean accuracy of the classifier.\n",
    "\n",
    "We need to perform experiments for:\n",
    "-Yago\n",
    "-Dbpedia\n",
    "-FB25K-137\n",
    "\n",
    "Models:\n",
    "-RESCAL\n",
    "-TransE\n",
    "-Complex\n",
    "-DistMult\n",
    "-ConvE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame(columns=['Dataset', 'Embedding','Experiment','Classifier', 'Precision', 'Recall', 'F1-Measure'])\n",
    "\n",
    "\n",
    "MIN_CLASS_SIZE = 40\n",
    "\n",
    "# Path to embedding files.\n",
    "#Embeddings are from LibKGE: https://github.com/uma-pi1/kge\n",
    "#RDF2Vec is from PyRDF2Vec: https://github.com/IBCNServices/pyRDF2Vec/\n",
    "\n",
    "embeddings = {'Yago' : {\n",
    "             'Complex':'/embeddings/yago3-10-complex.pt',\n",
    "              'DistMult':'/embeddings/yago3-10-distmult.pt',\n",
    "              'ConvE':'/embeddings/yago3-10-conve.pt',\n",
    "                'RDF2Vec':'/embeddings/yago3-10-rdf2vec',\n",
    "              'TransE':'/embeddings/yago3-10-transe.pt',\n",
    "              'RESCAL':'/embeddings/yago3-10-rescal.pt'},\n",
    "              'Freebase':{\n",
    "                'RESCAL':'/embeddings/fb15k-237-rescal.pt',\n",
    "            'TransE':'/embeddings/fb15k-237-transe.pt',\n",
    "              'RDF2Vec':'/embeddings/fb15k-237-rdf2vec',\n",
    "              'Complex':'/embeddings/fb15k-237-complex.pt',\n",
    "              'DistMult':'/embeddings/fb15k-237-distmult.pt',\n",
    "              'ConvE':'/embeddings/fb15k-237-conve.pt'}\n",
    "             }\n",
    "    \n",
    "datasets = {'Yago' : '/yago3-10/yago3-10TransitiveType.tsv',\n",
    "           'Freebase' : '/fb15k-237/freebaseTypes.tsv'}\n",
    "\n",
    "\n",
    "# Classes used in our experiments\n",
    "experiments = {'Yago': {\n",
    "              'Level-1': ['wordnet_person_100007846', 'wordnet_organization_108008335', 'wordnet_body_of_water_109225146', 'wordnet_product_104007894'],\n",
    "              'Level-2-Organizations': ['wordnet_musical_organization_108246613', 'wordnet_party_108256968', 'wordnet_enterprise_108056231', 'wordnet_nongovernmental_organization_108009834'],  \n",
    "               'Level-2-Waterbodies': ['wordnet_stream_109448361', 'wordnet_lake_109328904', 'wordnet_ocean_109376198', 'wordnet_bay_109215664', 'wordnet_sea_109426788'],\n",
    "               'Level-2-Persons': ['wordnet_artist_109812338', 'wordnet_officeholder_110371450', 'wordnet_writer_110794014', 'wordnet_scientist_110560637', 'wordnet_politician_110450303'], \n",
    "               'Level-3-Writers': ['wordnet_journalist_110224578', 'wordnet_poet_110444194', 'wordnet_novelist_110363573', 'wordnet_scriptwriter_110564905', 'wordnet_dramatist_110030277', 'wordnet_essayist_110064405', 'wordnet_biographer_109855433'], \n",
    "               'Level-3-Scientists': ['wordnet_social_scientist_110619642', 'wordnet_biologist_109855630', 'wordnet_physicist_110428004', 'wordnet_mathematician_110301261', 'wordnet_chemist_109913824', 'wordnet_linguist_110264437', 'wordnet_psychologist_110488865', 'wordnet_geologist_110127689', 'wordnet_computer_scientist_109951070', 'wordnet_research_worker_110523076'], \n",
    "              'level-3-Players': ['wordnet_football_player_110101634','wordnet_ballplayer_109835506','wordnet_soccer_player_110618342','wordnet_volleyball_player_110759047','wordnet_golfer_110136959'],\n",
    "               'Level-3-Artists': ['wordnet_painter_110391653', 'wordnet_sculptor_110566072', 'wordnet_photographer_110426749', 'wordnet_illustrator_109812068', 'wordnet_printmaker_110475687']\n",
    "                    },\n",
    "               'Freebase':{\n",
    "                'Level-1': ['wordnet_person_100007846', 'wordnet_organization_108008335', 'wordnet_body_of_water_109225146', 'wordnet_product_104007894'],\n",
    "               'Level-2-Organizations': ['wordnet_musical_organization_108246613', 'wordnet_party_108256968', 'wordnet_enterprise_108056231', 'wordnet_nongovernmental_organization_108009834'],  \n",
    "               'Level-2-Persons': ['wordnet_artist_109812338', 'wordnet_officeholder_110371450', 'wordnet_writer_110794014', 'wordnet_scientist_110560637', 'wordnet_politician_110450303'],         \n",
    "               'Level-3-Artists': ['wordnet_painter_110391653', 'wordnet_sculptor_110566072', 'wordnet_photographer_110426749', 'wordnet_illustrator_109812068', 'wordnet_printmaker_110475687']           \n",
    "                    }\n",
    "               }\n",
    "\n",
    "for dataset in embeddings.keys():\n",
    "\n",
    "    #load classes\n",
    "    try:\n",
    "        class_entity_dict, entity_class_dict = get_types(datasets[dataset], dataset)\n",
    "    except:\n",
    "        print(\"Error reading classes\")\n",
    "        continue\n",
    "        \n",
    "    print(\"Loaded dataset: {}\".format(dataset))\n",
    "    for embedding in embeddings[dataset]:\n",
    "        embedding_path = embeddings[dataset][embedding]\n",
    "        #load embedding from file\n",
    "        try:\n",
    "            model, entity_list = load_embedding(embedding_path)\n",
    "        except Exception as e:\n",
    "            print(str(e))\n",
    "            break\n",
    "        print(\"Loaded embedding: {}\".format(embedding))\n",
    "    \n",
    "        for e in experiments[dataset].keys():\n",
    "            input_classes = experiments[dataset][e]\n",
    "            training_ids = []\n",
    "\n",
    "            \n",
    "            # label array annotating each entity with classes\n",
    "            labels = []\n",
    "            print(\"Embedding {} on Dataset {} in Experiment {} with {} training examples:\".format(embedding, dataset,e ,len(entity_list)))\n",
    "            for c in input_classes:\n",
    "                class_size_counter = 0\n",
    "                for i, entity in enumerate(entity_list):\n",
    "                    #if entity in class_entity_dict[c] and entity in new_entity_list:\n",
    "                    if entity in class_entity_dict[c]:\n",
    "                        class_size_counter += 1\n",
    "                        #training_ids.append(i)\n",
    "                        try:\n",
    "                            labels[i].append(1)\n",
    "                        except IndexError:\n",
    "                            labels.append([])\n",
    "                            labels[i].append(1)\n",
    "                    else:\n",
    "                        #training_ids.append(i)\n",
    "                        try:\n",
    "                            labels[i].append(0)\n",
    "                        except IndexError:\n",
    "                            labels.append([])\n",
    "                            labels[i].append(0)\n",
    "\n",
    "                print(\"Class {} has length: {}\".format(c, class_size_counter))\n",
    "                #delete small classes \n",
    "                if class_size_counter < MIN_CLASS_SIZE:\n",
    "                    print(\"Class {} is too small and will be deleted\".format(c))\n",
    "                    for i, entity in enumerate(entity_list):\n",
    "                        del labels[i][-1]\n",
    "                    \n",
    "            # clean out entities which do not belong to any of the input classes\n",
    "            for i, entity in enumerate(entity_list):\n",
    "                for label in labels[i]:\n",
    "                    if label == 1:\n",
    "                        training_ids.append(i)\n",
    "                        break\n",
    "       \n",
    "            labels_tmp = (labels[i] for i in training_ids)\n",
    "            labels = list(labels_tmp)\n",
    "\n",
    "            if len(training_ids) != len(labels):\n",
    "                print(\"Error\")\n",
    "            else:\n",
    "                print(\"{} no of entities left.\".format(len(training_ids)))\n",
    "                #skip experiment if too small\n",
    "                if len(training_ids) < 10:\n",
    "                    continue\n",
    "\n",
    "            from sklearn import svm\n",
    "            from sklearn.model_selection import train_test_split\n",
    "            if \"RDF\" in embedding:\n",
    "                X = model[training_ids]\n",
    "            else:\n",
    "                train_id_tensor = torch.Tensor(training_ids).long() \n",
    "                X = model.get_s_embedder().embed(train_id_tensor).tolist()\n",
    "            y = labels\n",
    "            \n",
    "            #oversampling?\n",
    "            X_sample = []\n",
    "            y_sample = []\n",
    "\n",
    "            #split into training and testset\n",
    "            X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=37)\n",
    "\n",
    "\n",
    "           \n",
    "        #MLP Classifier\n",
    "            print(\"Results for MLP Classifier\")\n",
    "            from sklearn.neural_network import MLPClassifier\n",
    "            clf = MLPClassifier(max_iter=1000).fit(X_train, y_train)\n",
    "            print(clf.score(X_test, y_test))\n",
    "            y_pred = clf.predict(X_test)\n",
    "            print(precision_recall_fscore_support(y_test, y_pred))\n",
    "            print(\"Averaged Precision: {}\".format(precision_score(y_test, y_pred, average='weighted')))\n",
    "            print(\"Averaged Recall: {}\".format(recall_score(y_test, y_pred, average='weighted')))\n",
    "            print(\"Averaged F1: {}\".format(f1_score(y_test, y_pred, average='weighted')))\n",
    "            mlp_precision = round((precision_score(y_test, y_pred, average='weighted')*100),1)\n",
    "            mlp_recall = round((recall_score(y_test, y_pred, average='weighted')*100),1)\n",
    "            mlp_f1 = round((f1_score(y_test, y_pred, average='weighted')*100),1)\n",
    "        #Random Forest Classifier\n",
    "            print(\"Results for Random Forest Classifier\")\n",
    "            from sklearn.ensemble import RandomForestClassifier\n",
    "            clf = RandomForestClassifier()\n",
    "            clf.fit(X_train, y_train)\n",
    "            print(clf.score(X_test, y_test))\n",
    "            y_pred = clf.predict(X_test)\n",
    "            print(precision_recall_fscore_support(y_test, y_pred))\n",
    "            rf_precision = round((precision_score(y_test, y_pred, average='weighted')*100),1)\n",
    "            rf_recall = round((recall_score(y_test, y_pred, average='weighted')*100),1)\n",
    "            rf_f1 = round((f1_score(y_test, y_pred, average='weighted')*100),1)\n",
    "            \n",
    "        #K Nearest Neighbors\n",
    "            print(\"Results for KNN Classifier\")\n",
    "            from sklearn.neighbors import KNeighborsClassifier\n",
    "            clf = KNeighborsClassifier()\n",
    "            clf.fit(X_train, y_train)\n",
    "            print(clf.score(X_test, y_test))\n",
    "            y_pred = clf.predict(X_test)\n",
    "            print(precision_recall_fscore_support(y_test, y_pred))\n",
    "            knn_precision = round((precision_score(y_test, y_pred, average='weighted')*100),1)\n",
    "            knn_recall = round((recall_score(y_test, y_pred, average='weighted')*100),1)\n",
    "            knn_f1 = round((f1_score(y_test, y_pred, average='weighted')*100),1)\n",
    "            print(\" {} & {} & {} & {} & {} & {} & {} & {} & {}\".format(mlp_precision, mlp_recall, mlp_f1, rf_precision, rf_recall, rf_f1, knn_precision, knn_recall, knn_f1 ))\n",
    "            \n",
    "            new_mlp = pd.Series({'Dataset':dataset, 'Embedding':embedding,'Experiment':e,'Classifier':'MLP', 'Precision':mlp_precision, 'Recall':mlp_recall, 'F1-Measure':mlp_f1})\n",
    "            new_rf = pd.Series({'Dataset':dataset, 'Embedding':embedding,'Experiment':e,'Classifier':'Random Forest', 'Precision':rf_precision, 'Recall':rf_recall, 'F1-Measure':rf_f1})\n",
    "            new_knn = pd.Series({'Dataset':dataset, 'Embedding':embedding,'Experiment':e,'Classifier':'KNN', 'Precision':knn_precision, 'Recall':knn_recall, 'F1-Measure':knn_f1})\n",
    "            df = df.append(new_mlp, ignore_index=True)\n",
    "            df = df.append(new_rf, ignore_index=True)\n",
    "            df = df.append(new_knn, ignore_index=True)\n",
    "            df['F1-Measure'] = df['F1-Measure'].mul(100).round(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ADD results from baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sd_type = pd.Series({'Dataset': 'Freebase', 'Embedding':'SDType','Experiment':'Level-1','Classifier':'SDType', 'Precision':0.00, 'Recall':0.00, 'F1-Measure':96.8})\n",
    "df = df.append(sd_type, ignore_index=True)\n",
    "sd_type = pd.Series({'Dataset': 'Freebase', 'Embedding':'SDType','Experiment':'Level-2-Organizations','Classifier':'SDType', 'Precision':0.00, 'Recall':0.00, 'F1-Measure':86.6})\n",
    "df = df.append(sd_type, ignore_index=True)\n",
    "sd_type = pd.Series({'Dataset': 'Freebase', 'Embedding':'SDType','Experiment':'Level-2-Persons','Classifier':'SDType', 'Precision':0.00, 'Recall':0.00, 'F1-Measure':57.4})\n",
    "df = df.append(sd_type, ignore_index=True)\n",
    "sd_type = pd.Series({'Dataset': 'Freebase', 'Embedding':'SDType','Experiment':'Level-3-Artists','Classifier':'SDType', 'Precision':0.00, 'Recall':0.00, 'F1-Measure':86.1})       \n",
    "df = df.append(sd_type, ignore_index=True)\n",
    "                    \n",
    "\n",
    "sd_type = pd.Series({'Dataset': 'Yago', 'Embedding':'SDType','Experiment':'Level-1','Classifier':'SDType', 'Precision':0.00, 'Recall':0.00, 'F1-Measure': 98.3})       \n",
    "df = df.append(sd_type, ignore_index=True)\n",
    "sd_type = pd.Series({'Dataset': 'Yago', 'Embedding':'SDType','Experiment':'Level-2-Organizations','Classifier':'SDType', 'Precision':0.00, 'Recall':0.00, 'F1-Measure': 75.1})       \n",
    "df = df.append(sd_type, ignore_index=True)\n",
    "sd_type = pd.Series({'Dataset': 'Yago', 'Embedding':'SDType','Experiment':'Level-2-Waterbodies','Classifier':'SDType', 'Precision':0.00, 'Recall':0.00, 'F1-Measure':27.1})      \n",
    "df = df.append(sd_type, ignore_index=True)\n",
    "sd_type = pd.Series({'Dataset': 'Yago', 'Embedding':'SDType','Experiment':'Level-2-Persons','Classifier':'SDType', 'Precision':0.00, 'Recall':0.00, 'F1-Measure':58.6})       \n",
    "df = df.append(sd_type, ignore_index=True)\n",
    "sd_type = pd.Series({'Dataset': 'Yago', 'Embedding':'SDType','Experiment':'Level-3-Writers','Classifier':'SDType', 'Precision':0.00, 'Recall':0.00, 'F1-Measure':43.0})       \n",
    "df = df.append(sd_type, ignore_index=True)\n",
    "sd_type = pd.Series({'Dataset': 'Yago', 'Embedding':'SDType','Experiment':'Level-3-Players','Classifier':'SDType', 'Precision':0.00, 'Recall':0.00, 'F1-Measure':78.3})       \n",
    "df = df.append(sd_type, ignore_index=True)\n",
    "sd_type = pd.Series({'Dataset': 'Yago', 'Embedding':'SDType','Experiment':'Level-3-Scientists','Classifier':'SDType', 'Precision':0.00, 'Recall':0.00, 'F1-Measure':30.4})       \n",
    "df = df.append(sd_type, ignore_index=True)\n",
    "sd_type = pd.Series({'Dataset': 'Yago', 'Embedding':'SDType','Experiment':'Level-3-Artists','Classifier':'SDType', 'Precision':0.00, 'Recall':0.00, 'F1-Measure':40.4})       \n",
    "df = df.append(sd_type, ignore_index=True)\n",
    "                    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seaborn "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove other attributes from df\n",
    "df = df[['Dataset', 'Experiment', 'Embedding', 'Classifier', 'F1-Measure']]\n",
    "\n",
    "import seaborn as sns\n",
    "#sns.set_theme(style=\"whitegrid\")\n",
    "sns_plot = sns.relplot(x=\"F1-Measure\", y=\"Experiment\", hue=\"Embedding\", s=100, style='Classifier', data=df[df.Dataset=='Yago'].drop_duplicates(), aspect=2)\n",
    "fig = sns_plot.fig\n",
    "\n",
    "sns_plot = sns.relplot(x=\"F1-Measure\", y=\"Experiment\", hue=\"Embedding\", s=100, style='Classifier', data=df[df.Dataset=='Freebase'].drop_duplicates(), aspect=2)\n",
    "fig = sns_plot.fig\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "libge",
   "language": "python",
   "name": "libge"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
