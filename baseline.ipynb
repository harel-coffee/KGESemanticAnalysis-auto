{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Data for SDType and create input files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the type from the path files\n",
    "def get_types(type_file_path, ds):\n",
    "    if ds == \"yago\":\n",
    "        #read yago transitive types\n",
    "        from collections import defaultdict\n",
    "        class_entity_dict = defaultdict(set)\n",
    "        entity_class_dict = defaultdict(set)\n",
    "        with open(type_file_path, \"r\") as yago_types:\n",
    "            for line in yago_types:\n",
    "                try:\n",
    "                    x, entity, predicate, cl = line.split()\n",
    "                    entity = entity.replace(\">\",\"\").replace(\"<\",\"\")\n",
    "                    cl = cl.replace(\">\",\"\").replace(\"<\",\"\")\n",
    "                    class_entity_dict[cl].add(entity)\n",
    "                    entity_class_dict[entity].add(cl)\n",
    "                except ValueError:\n",
    "                    continue\n",
    "    if ds == \"freebase\":\n",
    "        from collections import defaultdict\n",
    "        class_entity_dict = defaultdict(set)\n",
    "        entity_class_dict = defaultdict(set)\n",
    "        with open(type_file_path, \"r\") as fb_types:\n",
    "            for line in fb_types:\n",
    "                try:\n",
    "                    entity, cl = line.split()\n",
    "                    class_entity_dict[cl].add(entity)\n",
    "                    entity_class_dict[entity].add(cl)\n",
    "                except ValueError:\n",
    "                    continue\n",
    "        \n",
    "    return class_entity_dict, entity_class_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Types for YAGO 3-10 Dataset from the original YAGO file.\n",
    "\n",
    "entity_set = set()\n",
    "with open (ntfiles[dataset],'r') as nt_file:\n",
    "    for line in nt_file:\n",
    "        s,p,o,_ = line.split()\n",
    "        entity_set.add(s.replace(\"<http://www.yago-knowledge.org/\",\"\").replace(\">\",\"\"))\n",
    "        entity_set.add(o.replace(\"<http://www.yago-knowledge.org/\",\"\").replace(\">\",\"\"))\n",
    "\n",
    "with open('/data/yago/yago3-10TransitiveType.tsv', \"w\") as yago310_types:\n",
    "    with open('/data/yago/yagoTransitiveType.tsv', \"r\") as yago_types:\n",
    "            for line in yago_types:\n",
    "                try:\n",
    "                    x, entity, predicate, cl = line.split()\n",
    "                    if entity.replace(\"<\",\"\").replace(\">\",\"\") in entity_set:\n",
    "                        yago310_types.write(line)\n",
    "                except ValueError:\n",
    "                    continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#insert file paths for type files, training datasets and output path for SDType training files.\n",
    "\n",
    "\n",
    "output_path = '/SDType/'\n",
    "\n",
    "ntfiles = {'yago' : '/data/yago/train.nt',\n",
    "           'freebase' : '/data/freebase/train.nt'}\n",
    "output_folder = {'yago' : '/data/yago/',\n",
    "           'freebase' : '/data/freebase/'}\n",
    "\n",
    "typefiles = {'yago' : '/data/yago/yago3-10TransitiveType.tsv',\n",
    "           'freebase' : '/data/freebase/freebaseTypes.tsv'}\n",
    "\n",
    "experiments = {'yago': {\n",
    "           'level-1': ['wordnet_person_100007846', 'wordnet_organization_108008335', 'wordnet_body_of_water_109225146', 'wordnet_product_104007894'],\n",
    "              'level-2-organization.txt': ['wordnet_musical_organization_108246613', 'wordnet_party_108256968', 'wordnet_enterprise_108056231', 'wordnet_nongovernmental_organization_108009834'],  \n",
    "               'level-2-waterbody.txt': ['wordnet_stream_109448361', 'wordnet_lake_109328904', 'wordnet_ocean_109376198', 'wordnet_bay_109215664', 'wordnet_sea_109426788'],\n",
    "               'level-2-person.txt': ['wordnet_artist_109812338', 'wordnet_officeholder_110371450', 'wordnet_writer_110794014', 'wordnet_scientist_110560637', 'wordnet_politician_110450303'], \n",
    "              'level-3-person-writer.txt': ['wordnet_journalist_110224578', 'wordnet_poet_110444194', 'wordnet_novelist_110363573', 'wordnet_scriptwriter_110564905', 'wordnet_dramatist_110030277', 'wordnet_essayist_110064405', 'wordnet_biographer_109855433'], \n",
    "              'level-3-person-scientist.txt': ['wordnet_social_scientist_110619642', 'wordnet_biologist_109855630', 'wordnet_physicist_110428004', 'wordnet_mathematician_110301261', 'wordnet_chemist_109913824', 'wordnet_linguist_110264437', 'wordnet_psychologist_110488865', 'wordnet_geologist_110127689', 'wordnet_computer_scientist_109951070', 'wordnet_research_worker_110523076'], \n",
    "             'level-3-person-player.txt': ['wordnet_football_player_110101634','wordnet_ballplayer_109835506','wordnet_soccer_player_110618342','wordnet_volleyball_player_110759047','wordnet_golfer_110136959'],\n",
    "              'level-3-person-artist.txt': ['wordnet_painter_110391653', 'wordnet_sculptor_110566072', 'wordnet_photographer_110426749', 'wordnet_illustrator_109812068', 'wordnet_printmaker_110475687']\n",
    "                    },\n",
    "               'freebase':{\n",
    "              'level-1': ['wordnet_person_100007846', 'wordnet_organization_108008335', 'wordnet_body_of_water_109225146', 'wordnet_product_104007894'],\n",
    "             'level-2-organization.txt': ['wordnet_musical_organization_108246613', 'wordnet_party_108256968', 'wordnet_enterprise_108056231', 'wordnet_nongovernmental_organization_108009834'],  \n",
    "               'level-2-person.txt': ['wordnet_artist_109812338', 'wordnet_officeholder_110371450', 'wordnet_writer_110794014', 'wordnet_scientist_110560637', 'wordnet_politician_110450303'], \n",
    "               'level-3-person-artist.txt': ['wordnet_painter_110391653', 'wordnet_sculptor_110566072', 'wordnet_photographer_110426749', 'wordnet_illustrator_109812068', 'wordnet_printmaker_110475687','wordnet_musician_110339966']           \n",
    "                    }\n",
    "               }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset in ntfiles.keys():\n",
    "    #load classes\n",
    "    try:\n",
    "        class_entity_dict, entity_class_dict = get_types(typefiles[dataset], dataset)\n",
    "    except Exception as e:\n",
    "        print(str(e))\n",
    "        break\n",
    "    #get entity list from file\n",
    "    entity_set = set()\n",
    "    with open (ntfiles[dataset],'r') as nt_file:\n",
    "        for line in nt_file:\n",
    "            s,p,o,_ = line.split()\n",
    "            if dataset == 'freebase':\n",
    "                entity_set.add(s.replace(\"<http://www.freebase.com\",\"\").replace(\">\",\"\"))\n",
    "                entity_set.add(o.replace(\"<http://www.freebase.com\",\"\").replace(\">\",\"\"))\n",
    "            else:\n",
    "                entity_set.add(s.replace(\"<http://www.yago-knowledge.org/\",\"\").replace(\">\",\"\"))\n",
    "                entity_set.add(o.replace(\"<http://www.yago-knowledge.org/\",\"\").replace(\">\",\"\"))\n",
    "            \n",
    "    entity_list = list(entity_set)\n",
    "    print(\"Loaded dataset: {}\".format(dataset))\n",
    "    \n",
    "    \n",
    "    for e in experiments[dataset].keys():\n",
    "        input_classes = experiments[dataset][e]\n",
    "        relevant_types = []\n",
    "        relevant_entities = set()\n",
    "        # label array annotating each entity with classes\n",
    "        labels = []\n",
    "        for c in input_classes:\n",
    "            print(\"Check for class {}\".format(c))\n",
    "            class_size_counter = 0\n",
    "            for i, entity in enumerate(entity_list):\n",
    "                #if entity in class_entity_dict[c] and entity in new_entity_list:\n",
    "                if entity in class_entity_dict[c]:\n",
    "                    type_nt_line = \"{}\\t{}\\n\".format(entity,c)\n",
    "                    relevant_types.append(type_nt_line)\n",
    "                    relevant_entities.add(entity)\n",
    "\n",
    "        #split into train and testset\n",
    "        random.shuffle(relevant_types)\n",
    "        train_length = int(len(relevant_types)*0.8)\n",
    "        train_data = relevant_types[:train_length]\n",
    "        test_data = relevant_types[train_length:]\n",
    "        with open (output_folder[dataset]+e+'train.nt','w') as train_file:\n",
    "            for line in train_data:\n",
    "                train_file.write(line)\n",
    "        with open (output_folder[dataset]+e+'test.nt','w') as test_file:\n",
    "            for line in test_data:\n",
    "                test_file.write(line)\n",
    "        with open (output_folder[dataset]+e+'training_triples.txt','w') as training_triples:\n",
    "            with open (ntfiles[dataset],'r') as nt_file:\n",
    "                for line in nt_file:\n",
    "                    s,p,o,_ = line.split()\n",
    "                    if dataset == 'freebase':\n",
    "                        if s.replace(\"<http://www.freebase.com\",\"\").replace(\">\",\"\") in relevant_entities or s.replace(\"<http://www.freebase.com\",\"\").replace(\">\",\"\") in relevant_entities:\n",
    "                            training_triples.write(\"{}\\t{}\\t{}\\n\".format(s.replace(\"<http://www.freebase.com\",\"\").replace(\">\",\"\"), p, s.replace(\"<http://www.freebase.com\",\"\").replace(\">\",\"\")))\n",
    "                    else:\n",
    "                        if s.replace(\"<http://www.yago-knowledge.org/\",\"\").replace(\">\",\"\") in relevant_entities or o.replace(\"<http://www.yago-knowledge.org/\",\"\").replace(\">\",\"\") in relevant_entities:\n",
    "                            training_triples.write(\"{}\\t{}\\t{}\\n\".format(s.replace(\"<http://www.yago-knowledge.org/\",\"\").replace(\">\",\"\"), p, o.replace(\"<http://www.yago-knowledge.org/\",\"\").replace(\">\",\"\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the Output Files\n",
    "\n",
    "The input to this code snipped is the output of SDType: https://github.com/HeikoPaulheim/sd-type-validate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "infiles = [\"level-1train.nt_sdtype.csv\",\"level-2-organization.txttrain.nt_sdtype.csv\",\"level-2-waterbody.txttrain.nt_sdtype.csv\",\"level-2-person.txttrain.nt_sdtype.csv\",\"level-3-person-writer.txttrain.nt_sdtype.csv\",\"level-3-person-scientist.txttrain.nt_sdtype.csv\",\"level-3-person-player.txttrain.nt_sdtype.csv\",\"level-3-person-artist.txttrain.nt_sdtype.csv\"]\n",
    "testfiles = [\"level-1test.nt\",\"level-2-organization.txttest.nt\",\"level-2-waterbody.txttest.nt\",\"level-2-person.txttest.nt\",\"level-3-person-writer.txttest.nt\",\"level-3-person-scientist.txttest.nt\",\"level-3-person-player.txttest.nt\",\"level-3-person-artist.txttest.nt\"]\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "df = pd.DataFrame(columns=['Experiment','Treshold','Precision', 'Recall'])\n",
    "from collections import defaultdict\n",
    "#infiles = [\"level-3-person-artist.txttrain.nt_sdtype.csv\"]\n",
    "#testfiles = [\"level-3-person-artist.txttest.nt\"]\n",
    "\n",
    "print(\"loading....\")\n",
    "#one loop per experiment\n",
    "for testfile, infile in zip(testfiles,infiles):\n",
    "    gold_data = set()\n",
    "    no_of_correct_results_perclass = defaultdict(lambda: 0)\n",
    "    testset_entities = set()\n",
    "    try:\n",
    "        with open ('/fb15k-237/'+testfile,'r') as test_file:\n",
    "            for line in test_file:\n",
    "                resource, t = line.split(\"\\t\")\n",
    "                testset_entities.add(resource)\n",
    "                t = t.replace(\"\\n\",\"\")\n",
    "                no_of_correct_results_perclass[t] += 1\n",
    "                gold_data.add(resource+t)\n",
    "    except FileNotFoundError:\n",
    "        print(\"Error\")\n",
    "        continue\n",
    "    print(\"Read test data\")\n",
    "    #read test dataset\n",
    "    \n",
    "    #one experiment per threshold\n",
    "    predictions = defaultdict(dict)\n",
    "    with open ('/fb15k-237/'+infile,'r') as prediction_file:\n",
    "        for line in prediction_file:\n",
    "            entity,e_class, confidence  = line.split(\" \")\n",
    "            entity = entity.replace(\">\",\"\").replace(\"<\",\"\")\n",
    "            e_class = e_class.replace(\">\",\"\").replace(\"<\",\"\")\n",
    "                \n",
    "            predictions[e_class][entity] = float(confidence)\n",
    "\n",
    "    print(\"Read prediction data\")\n",
    "    top_f1 = 0.0\n",
    "    for threshold in np.arange(0.0, 1, 0.001):\n",
    "        no_prediction_over_threshold = 0\n",
    "        avg_f1 = 0.0\n",
    "        avg_p = 0.0\n",
    "        avg_r = 0.0\n",
    "        #compute weight averaged f1 measure\n",
    "        for e_class in predictions.keys():\n",
    "            correct_in_sdtypes = 0\n",
    "            all_predictions = 0 \n",
    "            for entity, confidence in predictions[e_class].items():    \n",
    "                prediction = entity+e_class\n",
    "                if prediction in gold_data and confidence>= threshold:\n",
    "                    correct_in_sdtypes += 1\n",
    "                if confidence>= threshold and entity in testset_entities:\n",
    "                    all_predictions += 1\n",
    "            try:\n",
    "                p = correct_in_sdtypes/all_predictions\n",
    "                r = correct_in_sdtypes/no_of_correct_results_perclass[e_class]\n",
    "                f1 = (2*p*r)/(p+r)\n",
    "            except ZeroDivisionError:\n",
    "                #print(\"Error\")\n",
    "                p = 0\n",
    "                r = 0\n",
    "                f1= 0\n",
    "            #if f1 > 0.0 and p> 0 and r > 0:\n",
    "            no_prediction_over_threshold += no_of_correct_results_perclass[e_class]\n",
    "            avg_p += p*no_of_correct_results_perclass[e_class]\n",
    "            avg_r += r*no_of_correct_results_perclass[e_class]\n",
    "                \n",
    "            avg_f1 += f1*no_of_correct_results_perclass[e_class]\n",
    "            if f1 > 1.0:\n",
    "                raise ValueError(f\"F1 should not be {f1}\")\n",
    "        if no_prediction_over_threshold >0:\n",
    "            avg_f1 = avg_f1/no_prediction_over_threshold\n",
    "            avg_p = avg_p/no_prediction_over_threshold\n",
    "            avg_r = avg_r/no_prediction_over_threshold\n",
    "            new_row = pd.Series({'Experiment':testfile.replace(\"test.nt\",\"\"),'Treshold': threshold, 'overTH':no_prediction_over_threshold, 'Precision': avg_p, 'Recall': avg_r})\n",
    "            df = df.append(new_row, ignore_index=True)\n",
    "        \n",
    "        #only keep if the current f1 measure is better than the existing one\n",
    "        if avg_f1>= top_f1:\n",
    "            top_f1 = avg_f1\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_new = df[['Experiment','Precision', 'Recall']].drop_duplicates()\n",
    "df_new = df_new.replace({'level-1': 'Level-1'})\n",
    "df_new = df_new.replace({'level-2-organization.txt': 'Level-2-Organizations'})\n",
    "df_new = df_new.replace({'level-2-waterbody.txt': 'Level-2-Waterbodies'})\n",
    "df_new = df_new.replace({'level-2-person.txt': 'Level-2-Persons'})\n",
    "df_new = df_new.replace({'level-3-person-writer.txt': 'Level-3-Writers'})\n",
    "df_new = df_new.replace({'level-3-person-scientist.txt': 'Level-3-Scientists'})\n",
    "df_new = df_new.replace({'level-3-person-player.txt': 'Level-3-Players'})\n",
    "df_new = df_new.replace({'level-3-person-artist.txt': 'Level-3-Artists'})\n",
    "\n",
    "\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "ax = sns.lineplot(data=df_new, x=\"Recall\", y=\"Precision\", hue=\"Experiment\", ci=None, markers=True)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "libge",
   "language": "python",
   "name": "libge"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
